---
title: "Pre-processing"
author: "Rochelle Terman"
date: "August 2015"
output: html_document
---

### Required Pakcages

First let's load our required packages.

```{r}
setwd("~/Dropbox/berkeley/Dissertation/Data and Analyais/Git Repos/text-analysis-dhbsi")
rm(list=ls())
library(tm) # Framework for text mining
library(RTextTools) # a machine learning package for text classification written in R
library(qdap) # Quantiative discourse analysis of transcripts
library(qdapDictionaries)
library(dplyr) # Data preparation and pipes $>$
library(ggplot2) # for plotting word frequencies
library(SnowballC) # for stemming
```

## 1. Prepare a Corpus

A corpus is a collection of texts, usually stored electronically, and from which we perform our analysis. A corpus might be a collection of news articles from Reuters or the published works of Shakespeare. 

Within each corpus we will have separate articles, stories, volumes, each treated as a separate entity or record. Each unit is called a "document."

Documents come in a variety of formats, but plain text is best.

For this unit, we will be using a section of Machiavelli's Prince as our corpus. Since The Prince is a monograph, we have already "chunked" the text, so that each short paragraph or "chunk" is considered a "document."

### 1.1 Corpus Sources and Readers

The `tm` package supports a variety of sources and formats. 

```{r}
getSources()
getReaders()
```

Here we'll be going over two main options to input a corpus. 

Option 1) We can read a corpus from a directory that contains text files, each document a different file.
```{r}
docs <- Corpus(DirSource("Data/MachText")) 
docs
```

Option 2) We can read from a csv of documents, with each row being a document, and columns for text and metadata (information about each document). This is the easiest option if you have metadata.

```{r}
docs.df <-read.csv("Data/mach.csv", header=TRUE) #read in CSV file
docs <- Corpus(VectorSource(docs.df$text))
docs
```

Once we have the corpus, we can inspect the documents using inspect()

```{r}
# see the 16th document
inspect(docs[16])
```

And see the text using the `as.chracter`

```{r}
 # see content for 16th document
as.character(docs[[16]])
```

### 1.2 Preprocessing

Many text analysis applications follow a similar 'recipe' for preprecessing, involving:

1. Tokenizing the text to unigrams (or bigrams, or trigrams)
2. Converting all characters to lowercase
3. Removing punctuation
4. Removing numbers
5. Removing Stop Words, inclugind custom stop words
6. "Stemming" words, or lemmitization. There are several stemming alogrithms. Porter is the most popular.
7. Weighting features
8. Creating a Document-Term Matrix
9. Removing Sparse Terms

See what transformations are available TM package

```{r}
getTransformations()
```

The function `tm_map()` is used to apply one of these transformations across all documents. 

We can also use regular R functions for custom transformations using the `content_transformer()` function.

For example, we might want to replace “/”, used sometimes to separate alternative words, with a space. This will avoid the two words being run into one string of characters through the transformations. We might also replace “@” and “|” with a space, for the same reason
```{r}
# note: won't work on past versions of R
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace,"\\|")
as.character(docs[[16]])
```

We can basic transformations using the build in `tm_map` tools.

```{r}
docs <- tm_map(docs, content_transformer(tolower)) # convert all text to lower case
as.character(docs[[16]])
docs <- tm_map(docs, removePunctuation) # remove Puncturation
as.character(docs[[16]])
docs <- tm_map(docs, removeNumbers) # remove Numbers
as.character(docs[[16]])
docs <- tm_map(docs, removeWords, stopwords("english")) # remove common words
stopwords("english") # check out what was removed
as.character(docs[[16]])
docs <- tm_map(docs, removeWords, c("prince")) # remove own stop words
as.character(docs[[16]])
docs <- tm_map(docs, stripWhitespace) # strip white space
as.character(docs[[16]])
docs <- tm_map(docs, stemDocument) # stem the document
as.character(docs[[16]])
```
### 1.3 Creating a DTM

A document term matrix is simply a matrix with documents as the rows and terms as the columns and a count of the frequency of words as the cells of the matrix. We use `DocumentTermMatrix()` to create the matrix:
```{r}
dtm <- DocumentTermMatrix(docs)
dtm
```

`tm` also lets us convert a corpus to a DTM while completing the pre-processing steps in one step.

```{r}
dtm <- DocumentTermMatrix(docs,
           control = list(stopwords = TRUE,
                          tolower = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE,
                          stemming=TRUE))
```

One common pre-processing step that some applicaitons may call for is applying tf-idf weights. The [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), or term frequency-inverse document frequency, is a weight that ranks the importance of a term in its contextual document corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. In other words, it places importance on terms frequent in the document but rare in the corpus.

We won't be using tf-idf weghts for the rest of the unit, but you should know about it for other applications.

```{r}
dtm.weighted <- DocumentTermMatrix(docs,
           control = list(weighting =function(x) weightTfIdf(x, normalize = TRUE),
                          stopwords = TRUE,
                          tolower = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE,
                          stemming=TRUE))
```

### 1.4 Exploring the DTM

Let's look at the structure of our DTM.

```{r}
# how many documents? how many terms?
dim(dtm)
# take a quick look
inspect(dtm[1:5,1000:1005])
inspect(dtm.weighted[1:5,1000:1005])
```

We can obtain the term frequencies as a vector by converting the document term matrix into a matrix and summing the column counts:

```{r}
 # how many terms?
freq <- colSums(as.matrix(dtm))
length(freq)
```

By ordering the frequencies we can list the most frequent terms and the least frequent terms:

```{r}
# order
ord <- order(freq)

# Least frequent terms
freq[head(ord)]

# most frequent
freq[tail(ord)]

# frequency of frenquencies
head(table(freq),15)
tail(table(freq),15)

# plot
plot(table(freq))
```

**Bonus**: reorder columns of DTM to show most frequent terms first

```{r}
dtm.ordered <- dtm[,order(freq, decreasing = T)]
inspect(dtm.ordered[1:5,1:5])
```
Exploring word frequences

```{r}
# Have a look at common words
findFreqTerms(dtm, lowfreq=100) # words that appear at least 100 times

# Which words correlate with "war"?
findAssocs(dtm, "war", 0.3)

# plot the most frequent words
freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)
head(freq)

wf <- data.frame(word=names(freq), freq=freq)
head(wf)

subset(wf, freq>50) %>%
  ggplot (aes(word, freq)) +
  geom_bar (stat ="identity") +
  theme(axis.text.x=element_text(angle=45,hjust=1))

# wordcoulds!
library(wordcloud)
set.seed(123)
wordcloud(names(freq), freq, max.words=100, colors=brewer.pal(6,"Dark2"))
```
We can remove sparse terms and thus inrease efficency

```{r}
dtm.s <- removeSparseTerms(dtm,.99)
dtm # 2365 terms
dtm.s # 1326 terms
```

### 1.5 Exporting the DTM

We can convert a DTM to a matrix or data.frame in order to write to a csv, add meta data, etc.

```{r}
# coerce into dataframe
dtm <- as.data.frame(as.matrix(dtm))
names(docs)  # names of documents

# add fake column for section
dtm$doc_section <- "NA"
dtm$doc_section[1:100] <- "Section 1"
dtm$doc_section[101:188] <- "Section 2"
dtm$doc_section <- as.factor(dtm$doc_section)

# check to see if they're the same number of documents per author
summary(dtm$doc_section)
```

## 2. Bonus: rTextTools

FYI: We can use `rTextTools` to go directly from dataframe to DTM, completing pre-processing stems automatically. If you read your corpus using Option 1 above (Section 1.1) you must first convert your corpus into a dataframe.

```{r}

# Read corpus directly from dir and convert to data.frame.
docs <- Corpus(DirSource("Data/MachText"))
docs.df<-data.frame(text=unlist(sapply(docs, `[`, "content")), 
    stringsAsFactors=F)
```

If you read in your corpus as a CSV, you should already have a dataframe (in this case, `docs.df`) in your environment.

```{r}
# Read corpus from csv
docs.df <- read.csv("Data/mach.csv")
```

Now we can make out DTM in one step.

```{r}
# Convert to DTM
dtm <- create_matrix(docs.df$text, language="english", removeNumbers=TRUE,
                       stemWords=TRUE, toLower = TRUE, removePunctuation = TRUE)
```

### Execise: Make a DTM out of your own corpus or one of the toy copora.

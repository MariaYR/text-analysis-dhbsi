---
title: "Literary Concerns and Word Order Dependent Methods"
author: "Teddy Roland"
date: "August 2015"
output: html_document
---

### Required Packages

```{r}
rm(list=ls())
setwd("~/Dropbox/berkeley/Dissertation/Data\ and\ Analyais/Git\ Repos/text-analysis-dhbsi")

library(tm)
library(RTextTools) # a machine learning package for text classification written in R
library(qdap) # quantiative discourse analysis
library(entropy) # tools applying Information Theory

```

## 1. Alliteration: Counting

Unlike many computational studies on novels, when we look at poetry -- and especially poetic form -- we are often interested not in the words themselves but their sounds. Essentially, this is the difference between orthography and phonology. Although there are many languages in which there is a one-to-one mapping between letters and sounds, this is not the case for English. Fortunately, there are several publicly available pronouncing dictionaries, such as the CMU Pronouncing Dictionary and Nettalk. These dictionaries include information about syllable units, stress, and phonemes -- and in some cases multiple pronunciations.

Of course, when we study poetic form we are interested not only in sounds but their sequences and repetitions, which means that a bag-of-words model might only have limited application. In practice, the methods used to study poetic form typically begin with dictionary lookup methods (not unlike we did with sentiment analysis) and incorporate some kind of pattern recognition, increasingly using statistics. This lesson will walk through some of those lookups and start to think about strategies for finding patterns.

```{r}
# Load Data
sonnets.df <- read.csv('Data/shakes-sonnets.csv', header=FALSE, sep="\t", as.is=TRUE)

# Take a peek: Each row contains one sonnet and each column contains one line
head(truncdf(sonnets.df),10)
```

"unlist" is a function to take entries from a two-dimensional structures like a data frame and turn them into a one-dimensional list. This will come in handy when we want to perform an operation on every entry.

```{r}
# Get a list of all lines in the sonnets
all_lines = unlist(sonnets.df)
```

"syllable_sum" in the qdap package is a powerful function for syllable counting. It first checks the Nettalk pronouncing dictionary to check whether it knows the number of syllables already, and failing that, uses machine learning to guess the number of syllables based on orthography.

```{r}
# Count the number of syllables per line in all sonnets
all_sylls = syllable_sum(all_lines)

# Check results
all_sylls
```

Shakespeare's sonnets are a famous and extensive example of iambic pentameter, which is in part characterized by a ten syllable line. We can check the consistency of that syllable count, using a couple basic statistics.

Note that when we checked all_sylls, r had introduced an "NA" wherever it had expected to find a line of poetry but didn't (since the number of lines varies across in the cycle). We will remove these before our statistical measures.

```{r}
# Median syllable count (Half of all counts above, half below)
median(all_sylls, na.rm=TRUE)

# Inter-Quartile Range (Difference across the middle fifty-percent of all counts)
IQR(all_sylls, na.rm=TRUE)
```
Let's look more closely at just one sonnet to see what's happening under the hood.

```{r}
# Get a list of lines in the eighth sonnet
eighth_sonn = unlist(sonnets.df[8,])
```

"syllable_sum" relies on a broader function called "syllable_count" that gives some more information.

```{r}
# Get syllable information for just the first line.
syllable_count(eighth_sonn[[1]])
```

Okay, now let's break the function. "syllable_sum" is very good at counting syllables, but let's say we wanted to study alliteration in the sonnets, which requires a little more information about pronunciations.

The documentation indicates that the function begins with a dictionary lookup from the "Nettalk Corpus", that is publicly available on the internet.

```{r}
# Documentation for syllable_sum
help(syllable_sum)

# Load the Nettalk dictionary
nettalk.df <- read.csv('nettalk.data.txt', header=FALSE, sep="\t", as.is=TRUE)
```

If we're going to perform our own lookups in the dictionary, we have to do some legwork that "syllable_sum" had done under the hood.

```{r}
# Tokenize the first line of the eighth sonnet
tokens <- sapply(eighth_sonn[[1]], function(x) gsub("[[:punct:]]", "", tolower(MC_tokenizer(x))) )

# Look up the entry for each word
pronunciations <- sapply(tokens, function(x) nettalk.df[nettalk.df[1]==as.vector(x)][2])

# Check the results
pronunciations
```

Now that we have information about the pronunciation about *most* syllables, we can start to think about alliteration.

```{r}
# Get the first phoneme of each word
first.phoneme <- sapply(pronunciations, function(x) substr(x,1,1))

# Count the frequency of each phoneme
table(first.phoneme)
```


## 2. Part-of-Speech Tagging

Part of Speech tagging is used for a great variety of tasks, including Named Entity Recognition (NER) which can help to identify characters in a text as well as higher order problems like determining who is the subject or object of an action. Another type of question we can ask is about the grammatical sophistication of a text. One version of this is treated as the text's "reading level," while in literature this may be thought of as interpreting an author's treatment of a subject.

```{r}
# Load corpus of novels
documents<-Corpus(DirSource("Data/POS-fiction"))

# Take a peek at the novels
# The double brackets call up different novels: 1 = A Game of Thrones; 2 = Beloved
head(as.character(documents$content[[1]]))
```

Importing the novel had introduced section breaks across paragraphs that we don't need, but on the other hand, we want to split the text into sentences. 

```{r}
# Combine sections of text
one.string <- paste(documents$content[[1]], collapse = " ")

# Split into sentences
sentences <- sent_detect(one.string)
```

Now that the text is divided into more grammatically meaningful units, we can start looking at Parts of Speech.

```{r}
# Run Part-of-Speech tagger over the novels
posd <- pos(sentences)
# if pressed for time
posd <- pos(head(sentences))
```

The "pos" function returns a great deal of information, including not only the text itself, but full sentences in which each word appears with its tag, and counts of the tags themselves. We're especially interested in the kind of information reported in the value "POStagged".

```{r}
# visualize parts of speech per sentence
plot(posd)

# view sentences with tags in-line, list of tags by sentence, & word counts
posd$POStagged
```

The in-line tags might be useful if we wished to, say, idenitify the frequencies of nouns across our texts. For this particular project, we'll use the list of tags to compute bigram frequencies.

```{r}
# Get list of tags for each sentence
posd.list <- posd[[2]][[2]]

# Get bigrams of POS tags
pos.grams <- sapply(posd.list, function(x) ngrams(paste(x, collapse = " "), 2))
```

We're juggling a list of lists that is slightly easier to use as a Data Frame, but we need to get to a place where we can easily count up each instance of all bigrams.

```{r}
pos.frame<-as.data.frame(pos.grams)

# Get the list of bigrams for each sentence
gram.list <-sapply(pos.frame, function(x) x$all_n$n_2)

# Pull out all the bigrams into a single list, since we no longer are interested in sentence divisions
gram.list <- unlist(gram.list, recursive=FALSE)

# Join bigrams with a hyphen into a single unit
gram.list<-sapply(gram.list, function(x) paste(x, sep=' ',collapse = '-'))

# Count the instances of all bigram units
gram.counts <- as.vector(table(gram.list))
```

And now that we have counted the total instances of each bigram, we can calculate the degree of grammatical entropy -- something like its complexity -- using Shannon's formula for informational entropy.

```{r}
entropy(gram.counts)
```
